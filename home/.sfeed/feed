#!/usr/bin/env sh

# Base directory for all sfeed-related data
SFEED_DIR="$HOME/.local/share/sfeed"

# Directory containing per-feed TSV files generated by sfeed
SFEED_FEED_DIR="$SFEED_DIR/feeds"

# Directory to store generated HTML files
SFEED_HTML_DIR="$SFEED_DIR/html"

# File containing URLs of entries already marked as read
SFEED_READ_FILE="$SFEED_DIR/read"

# File containing the entries currently being viewed (TSV)
SFEED_READING_FILE="$SFEED_DIR/reading"

# File containing `sfeed_update` command's output
SFEED_UPDATE_LOGFILE="$SFEED_DIR/sfeed_update.log"

# Print usage information and exit
usage() {
  echo "usage: $(basename "$0") [get|read|show|find|browser]"
}

# Filter unread entries from all feed files.
#
# Input:
#   - Reads the list of read URLs from $SFEED_READ_FILE
#   - Reads all feed TSV files under $SFEED_FEED_DIR
#
# Output:
#   - Writes unread entries as TSV to stdout
#
# Notes:
#   - Pure function: no side effects, no file writes
#   - Prepends feed filename to the title field for identification
filter_unread() {
  set -- "$SFEED_FEED_DIR"/*
  [ "$1" = "$SFEED_FEED_DIR/*" ] && return 0
  awk -F'\t' '
    FNR == NR {
      read[$0]
      next
    }
    !($3 in read) {
      fname = FILENAME
      sub(".*/", "", fname)
      $2 = "[" fname "] " $2
      out = $1
      for (i = 2; i <= NF; i++) {
        out = out "\t" $i
      }
      print out
    }
  ' "$SFEED_READ_FILE" "$@"
}

# Generate an HTML document from TSV input.
#
# Input:
#   - Reads TSV entries from stdin
#
# Output:
#   - Writes a complete HTML document to stdout
#
# Notes:
#   - Pure filter: stdin -> stdout
#   - Converts UNIX timestamps to human-readable date strings
generate_html() {
  cat <<EOF
<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="utf-8">
<title>sfeed</title>
<link rel="stylesheet" href="../style.css">
</head>
<body>
<ul class="feed">
EOF
  awk -F'\t' '
    function html_escape(s) {
      gsub(/&/,  "&amp;",  s)
      gsub(/</,  "&lt;",   s)
      gsub(/>/,  "&gt;",   s)
      gsub(/"/,  "&quot;", s)
      gsub(/'\''/, "&#39;", s)
      return s
    }
    {
      t = strftime("%Y-%m-%d %H:%M", $1)

      url = $3
      gsub(/localhost:8080/, "x.com", url)

      title = html_escape($2)
      url   = html_escape(url)

      printf "  <li class=\"item\">\n"
      printf "    <time class=\"date\">%s</time>\n", t
      printf "    <a class=\"title\" href=\"%s\">%s</a>\n", url, title
      printf "  </li>\n"
    }
  '
  cat <<EOF
</ul>
</body>
</html>
EOF
}

# Mark currently viewed entries as read.
#
# Input:
#   - Reads URLs from $SFEED_READING_FILE
#
# Side effects:
#   - Appends URLs to $SFEED_READ_FILE
#   - Deduplicates the read list in-place
markread() {
  cut -f3 "$SFEED_READING_FILE" >>"$SFEED_READ_FILE"
  sort -u "$SFEED_READ_FILE" -o "$SFEED_READ_FILE"
}

# Generate an HTML view of unread entries and open it in a browser.
#
# Flow:
#   - Filter unread entries
#   - Save them to $SFEED_READING_FILE
#   - Convert them to HTML
#   - Open the generated HTML file using $BROWSER
#
# Side effects:
#   - Creates an HTML file under $SFEED_HTML_DIR
#   - Writes to $SFEED_READING_FILE
open_with_browser() {
  mkdir -p "$SFEED_HTML_DIR"
  timestamp=$(date +%Y%m%d%H%M%S)
  html_file="$SFEED_HTML_DIR/sfeed-${timestamp}.html"

  filter_unread |
    tee "$SFEED_READING_FILE" |
    generate_html >"$html_file"

  "$BROWSER" "$html_file"
}

# Discover feed URLs from a given web page.
#
# Arguments:
#   $1 - URL to scan for feeds
#
# Output:
#   - Prints discovered feed URLs, one per line
find() {
  if [ -z "$1" ]; then
    echo "usage: $(basename "$0") find url"
    exit 1
  fi
  curl -sL "$1" | sfeed_web | cut -f1
}

# Require at least one command argument
[ -z "$1" ] && usage && exit 1

# Command dispatcher
case "$1" in
get)
  sfeed_update 2>&1 | tee -a "$SFEED_UPDATE_LOGFILE"
  ;;
read)
  markread
  ;;
show)
  filter_unread
  ;;
find)
  find "$2"
  ;;
browser)
  open_with_browser
  markread
  ;;
*)
  usage
  exit 1
  ;;
esac
